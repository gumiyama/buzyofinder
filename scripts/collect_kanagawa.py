#!/usr/bin/env python
import os
import sys
import time
from pathlib import Path
import requests

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.database import get_session, get_engine, Property, save_or_update_property
from src.scrapers.suumo_scraper import SuumoScraper

# ç¥å¥ˆå·çœŒï¼ˆæ­¦è”µå°æ‰ãƒ»é·ºæ²¼ï¼‰ã®è¨­å®š
AREAS = {
    'kawasakishinakahara': {'pages': 20, 'name': 'å·å´å¸‚ä¸­åŸåŒºï¼ˆæ­¦è”µå°æ‰ãªã©ï¼‰'},
    'kawasakishimiyamae': {'pages': 20, 'name': 'å·å´å¸‚å®®å‰åŒºï¼ˆé·ºæ²¼ãªã©ï¼‰'},
}

CRAWL_INTERVAL = 1.0

def save_property(url, session, scraper):
    """URLã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’å–å¾—ã—ã¦ä¿å­˜ã¾ãŸã¯æ›´æ–°"""
    try:
        source_id = url.split('/nc_')[1].split('/')[0] if '/nc_' in url else None
        if not source_id: return "skip"
        
        detail = scraper.get_property_detail(url)
        if not detail or not detail.get('price'): return "error"
        
        return save_or_update_property(session, detail, source_id)
        
    except Exception as e:
        print(f"      âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        return "error"

def process_area(area_code, config, session, scraper):
    """ç¥å¥ˆå·çœŒç‰ˆ: æŒ‡å®šã‚¨ãƒªã‚¢ã‚’å·¡å›"""
    # âš ï¸ ã“ã“ãŒé‡è¦: URLãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç¥å¥ˆå·çœŒç‰ˆ (kanagawa) ã«ãªã£ã¦ã„ã‚‹
    base_url = f'https://suumo.jp/ms/chuko/kanagawa/sc_{area_code}/'
    pages = config['pages']
    
    headers = {'User-Agent': 'Mozilla/5.0'}
    saved_count = 0
    
    for page in range(1, pages + 1):
        try:
            url = base_url if page == 1 else f"{base_url}?page={page}"
            print(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ {page}/{pages} ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­... ({config['name']})")
            
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code != 200:
                print(f"    âŒ HTTP {response.status_code}")
                # 404ãªã‚‰ãã®ãƒšãƒ¼ã‚¸ä»¥é™ã¯ãªã„å¯èƒ½æ€§ãŒé«˜ã„ãŒã€å¿µã®ãŸã‚continue
                continue
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.find_all('a', href=True)
            
            page_urls = set()
            for link in links:
                href = link['href']
                # ç¥å¥ˆå·URLãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã•ã›ã‚‹
                if '/ms/chuko/kanagawa/' in href and '/nc_' in href:
                    if not href.startswith('http'):
                        href = 'https://suumo.jp' + href
                    if 'bukkengaiyo' not in href:
                        href = href.split('?')[0].rstrip('/') + '/bukkengaiyo/'
                    page_urls.add(href)
            
            print(f"    ğŸ” {len(page_urls)}ä»¶ã®URLã‚’ç™ºè¦‹ã€‚ä¿å­˜é–‹å§‹...")
            
            for p_url in page_urls:
                result = save_property(p_url, session, scraper)
                if result == "saved":
                    saved_count += 1
                    print(f"      âœ… ä¿å­˜æˆåŠŸ: {p_url.split('/nc_')[1].split('/')[0]}")
            
            time.sleep(CRAWL_INTERVAL)
            
        except Exception as e:
            print(f"    âš ï¸ ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
            continue
            
    return saved_count

def main():
    print("=" * 60)
    print("ğŸš€ ç¥å¥ˆå·ï¼ˆæ­¦è”µå°æ‰ãƒ»é·ºæ²¼ï¼‰é›†ä¸­åé›†")
    print("=" * 60)
    
    engine = get_engine()
    session = get_session(engine)
    scraper = SuumoScraper(interval=1.0)
    
    total_saved = 0
    
    try:
        for area_code, config in AREAS.items():
            print(f"\n{config['name']} ã®å‡¦ç†ã‚’é–‹å§‹")
            count = process_area(area_code, config, session, scraper)
            total_saved += count
            print(f"  âœ¨ {config['name']} å®Œäº†: +{count}ä»¶")
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    finally:
        session.close()
        print("\n" + "=" * 60)
        print(f"ğŸ çµ‚äº†ã€‚æ–°è¦ä¿å­˜: {total_saved}ä»¶")

if __name__ == '__main__':
    main()

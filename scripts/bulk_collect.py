#!/usr/bin/env python
"""
å¤§é‡ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¤‡æ•°ã‚¨ãƒªã‚¢ã‹ã‚‰ç‰©ä»¶URLã‚’åé›†ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
"""

import os
import sys
import time
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.scrapers.suumo_scraper import SuumoScraper

# ã‚¨ãƒªã‚¢è¨­å®š
AREAS = {
    'chiyoda': {'pages': 15, 'name': 'åƒä»£ç”°åŒº', 'url': 'https://suumo.jp/ms/chuko/tokyo/sc_chiyoda/'},
    'shibuya': {'pages': 15, 'name': 'æ¸‹è°·åŒº', 'url': 'https://suumo.jp/ms/chuko/tokyo/sc_shibuya/'},
    'minato': {'pages': 15, 'name': 'æ¸¯åŒº', 'url': 'https://suumo.jp/ms/chuko/tokyo/sc_minato/'},
    'shinjuku': {'pages': 15, 'name': 'æ–°å®¿åŒº', 'url': 'https://suumo.jp/ms/chuko/tokyo/sc_shinjuku/'},
    'meguro': {'pages': 15, 'name': 'ç›®é»’åŒº', 'url': 'https://suumo.jp/ms/chuko/tokyo/sc_meguro/'},
}

INTERVAL = 3.0  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰


def collect_urls_for_area(area_code, area_config):
    """æŒ‡å®šã‚¨ãƒªã‚¢ã‹ã‚‰ç‰©ä»¶URLã‚’åé›†"""
    scraper = SuumoScraper()
    urls = set()
    
    base_url = area_config['url']
    pages = area_config['pages']
    
    for page in range(1, pages + 1):
        try:
            if page == 1:
                url = base_url
            else:
                url = f"{base_url}?page={page}"
            
            print(f"  ãƒšãƒ¼ã‚¸ {page}/{pages} ã‚’å–å¾—ä¸­... ", end='', flush=True)
            
            # HTMLã‚’å–å¾—
            html = scraper._fetch_html(url)
            if not html:
                print("âŒ ã‚¨ãƒ©ãƒ¼: HTMLã®å–å¾—ã«å¤±æ•—")
                continue
            
            # URLã‚’æŠ½å‡º
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')
            
            # ç‰©ä»¶ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            links = soup.find_all('a', href=True)
            page_urls = set()
            
            for link in links:
                href = link['href']
                if '/ms/chuko/tokyo/' in href and '/nc_' in href:
                    # å®Œå…¨URLã«å¤‰æ›
                    if not href.startswith('http'):
                        href = 'https://suumo.jp' + href
                    # bukkengaiyoã®URLã«å¤‰æ›
                    if 'bukkengaiyo' not in href:
                        href = href.split('?')[0]
                        if not href.endswith('/'):
                            href += '/'
                        href += 'bukkengaiyo/'
                    page_urls.add(href)
            
            urls.update(page_urls)
            print(f"âœ“ {len(page_urls)}ä»¶")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            if page < pages:
                time.sleep(INTERVAL)
                
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    return urls


def main():
    print("=" * 60)
    print("å¤§é‡ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬åé›†")
    print(f"å¯¾è±¡: {len(AREAS)}ã‚¨ãƒªã‚¢")
    print("=" * 60)
    
    all_urls = set()
    area_stats = {}
    
    # å„ã‚¨ãƒªã‚¢ã‹ã‚‰URLåé›†
    for area_code, config in AREAS.items():
        print(f"\nğŸ“ [{config['name']}] URLåé›†ä¸­...")
        urls = collect_urls_for_area(area_code, config)
        area_stats[config['name']] = len(urls)
        all_urls.update(urls)
        print(f"  âœ“ åˆè¨ˆ: {len(urls)}ä»¶")
    
    print("\n" + "=" * 60)
    print("URLåé›†å®Œäº†")
    print("=" * 60)
    
    # çµ±è¨ˆè¡¨ç¤º
    for area_name, count in area_stats.items():
        print(f"  {area_name}: {count}ä»¶")
    
    print(f"\né‡è¤‡é™¤å»å¾Œã®åˆè¨ˆ: {len(all_urls)}ä»¶")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    url_file = f"bulk_urls_{timestamp}.txt"
    
    with open(url_file, 'w') as f:
        for url in sorted(all_urls):
            f.write(url + '\n')
    
    print(f"\nğŸ’¾ {url_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—ç¢ºèª
    print("\n" + "=" * 60)
    response = input("ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/n): ")
    
    if response.lower() == 'y':
        print("\nãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹...")
        print("=" * 60)
        os.system(f"PYTHONPATH=. ./venv/bin/python scripts/fetch_from_url_file.py {url_file}")
    else:
        print(f"\nå¾Œã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã§ãã¾ã™:")
        print(f"  PYTHONPATH=. ./venv/bin/python scripts/fetch_from_url_file.py {url_file}")


if __name__ == '__main__':
    main()

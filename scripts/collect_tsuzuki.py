#!/usr/bin/env python
"""
æ¨ªæµœå¸‚éƒ½ç­‘åŒºï¼ˆã‚»ãƒ³ã‚¿ãƒ¼åŒ—ãƒ»ã‚»ãƒ³ã‚¿ãƒ¼å—ï¼‰é›†ä¸­åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import os
import sys
import time
from pathlib import Path
import requests

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.database import get_session, get_engine, Property
from src.scrapers.suumo_scraper import SuumoScraper

# æ¨ªæµœå¸‚éƒ½ç­‘åŒºã®è¨­å®šï¼ˆ30ãƒšãƒ¼ã‚¸ï¼‰
AREAS = {
    'yokohamashitsuzuki': {'pages': 30, 'name': 'æ¨ªæµœå¸‚éƒ½ç­‘åŒºï¼ˆã‚»ãƒ³ã‚¿ãƒ¼åŒ—ãƒ»ã‚»ãƒ³ã‚¿ãƒ¼å—ï¼‰'},
}

CRAWL_INTERVAL = 1.0

def save_property(url, session, scraper):
    """URLã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’å–å¾—ã—ã¦ä¿å­˜"""
    try:
        source_id = url.split('/nc_')[1].split('/')[0] if '/nc_' in url else None
        if not source_id: return "skip"
        
        existing = session.query(Property).filter_by(source_id=source_id).first()
        if existing: return "exists"
        
        detail = scraper.get_property_detail(url)
        if not detail or not detail.get('price'): return "error"
        
        property_obj = Property(
            source='SUUMO',
            source_id=source_id,
            url=url,
            title=detail.get('title') or f'ç‰©ä»¶ {source_id}',
            price=detail.get('price'),
            area=detail.get('area'),
            price_per_sqm=detail.get('price_per_sqm'),
            layout=detail.get('layout'),
            building_age=detail.get('building_age'),
            floor=detail.get('floor'),
            direction=detail.get('direction'),
            address=detail.get('address'),
            prefecture=detail.get('prefecture'),
            city=detail.get('city'),
            station_name=detail.get('station_name'),
            station_distance=detail.get('station_distance'),
            access_info=detail.get('access_info'),
            management_fee=detail.get('management_fee'),
            repair_reserve=detail.get('repair_reserve'),
            features=detail.get('features', '{}'),
            is_active=True
        )
        
        session.add(property_obj)
        session.commit()
        return "saved"
    except Exception as e:
        print(f"      âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        session.rollback()
        return "error"

def process_area(area_code, config, session, scraper):
    """ã‚¨ãƒªã‚¢ã”ã¨ã«ãƒšãƒ¼ã‚¸ã‚’å·¡å›ï¼ˆç¥å¥ˆå·çœŒç‰ˆï¼‰"""
    base_url = f'https://suumo.jp/ms/chuko/kanagawa/sc_{area_code}/'
    pages = config['pages']
    
    headers = {'User-Agent': 'Mozilla/5.0'}
    saved_count = 0
    
    for page in range(1, pages + 1):
        try:
            url = base_url if page == 1 else f"{base_url}?page={page}"
            print(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ {page}/{pages} ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­... ({config['name']})")
            
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code != 200:
                print(f"    âŒ HTTP {response.status_code}")
                continue
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.find_all('a', href=True)
            
            page_urls = set()
            for link in links:
                href = link['href']
                if '/ms/chuko/kanagawa/' in href and '/nc_' in href:
                    if not href.startswith('http'):
                        href = 'https://suumo.jp' + href
                    if 'bukkengaiyo' not in href:
                        href = href.split('?')[0].rstrip('/') + '/bukkengaiyo/'
                    page_urls.add(href)
            
            print(f"    ğŸ” {len(page_urls)}ä»¶ã®URLã‚’ç™ºè¦‹ã€‚ä¿å­˜é–‹å§‹...")
            
            for p_url in page_urls:
                result = save_property(p_url, session, scraper)
                if result == "saved":
                    saved_count += 1
                    print(f"      âœ… ä¿å­˜æˆåŠŸ: {p_url.split('/nc_')[1].split('/')[0]}")
            
            time.sleep(CRAWL_INTERVAL)
            
        except Exception as e:
            print(f"    âš ï¸ ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
            continue
            
    return saved_count

def main():
    print("=" * 60)
    print("ğŸš€ æ¨ªæµœå¸‚éƒ½ç­‘åŒºï¼ˆã‚»ãƒ³ã‚¿ãƒ¼åŒ—ãƒ»ã‚»ãƒ³ã‚¿ãƒ¼å—ï¼‰é›†ä¸­åé›†")
    print("=" * 60)
    
    engine = get_engine()
    session = get_session(engine)
    scraper = SuumoScraper(interval=1.0)
    
    total_saved = 0
    
    try:
        for area_code, config in AREAS.items():
            print(f"\n{config['name']} ã®å‡¦ç†ã‚’é–‹å§‹")
            count = process_area(area_code, config, session, scraper)
            total_saved += count
            print(f"  âœ¨ {config['name']} å®Œäº†: +{count}ä»¶")
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    finally:
        session.close()
        print("\n" + "=" * 60)
        print(f"ğŸ çµ‚äº†ã€‚æ–°è¦ä¿å­˜: {total_saved}ä»¶")

if __name__ == '__main__':
    main()

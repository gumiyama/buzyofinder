#!/usr/bin/env python
"""
å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ±äº¬23åŒºã™ã¹ã¦ã‹ã‚‰ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦500ä»¶ä»¥ä¸Šã‚’ç›®æŒ‡ã™
"""

import os
import sys
import time
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.database import get_session, get_engine, Property
from src.scrapers.suumo_scraper import SuumoScraper
import requests

# æ±äº¬23åŒºã®è¨­å®š
AREAS = {
    'chiyoda': {'pages': 10, 'name': 'åƒä»£ç”°åŒº'},
    'chuo': {'pages': 10, 'name': 'ä¸­å¤®åŒº'},
    'minato': {'pages': 10, 'name': 'æ¸¯åŒº'},
    'shinjuku': {'pages': 10, 'name': 'æ–°å®¿åŒº'},
    'bunkyo': {'pages': 10, 'name': 'æ–‡äº¬åŒº'},
    'taito': {'pages': 10, 'name': 'å°æ±åŒº'},
    'sumida': {'pages': 10, 'name': 'å¢¨ç”°åŒº'},
    'koto': {'pages': 10, 'name': 'æ±Ÿæ±åŒº'},
    'shinagawa': {'pages': 10, 'name': 'å“å·åŒº'},
    'meguro': {'pages': 10, 'name': 'ç›®é»’åŒº'},
    'ota': {'pages': 10, 'name': 'å¤§ç”°åŒº'},
    'setagaya': {'pages': 10, 'name': 'ä¸–ç”°è°·åŒº'},
    'shibuya': {'pages': 10, 'name': 'æ¸‹è°·åŒº'},
    'nakano': {'pages': 10, 'name': 'ä¸­é‡åŒº'},
    'suginami': {'pages': 10, 'name': 'æ‰ä¸¦åŒº'},
    'toshima': {'pages': 10, 'name': 'è±Šå³¶åŒº'},
    'kita': {'pages': 10, 'name': 'åŒ—åŒº'},
    'arakawa': {'pages': 10, 'name': 'è’å·åŒº'},
    'itabashi': {'pages': 10, 'name': 'æ¿æ©‹åŒº'},
    'nerima': {'pages': 10, 'name': 'ç·´é¦¬åŒº'},
    'adachi': {'pages': 10, 'name': 'è¶³ç«‹åŒº'},
    'katsushika': {'pages': 10, 'name': 'è‘›é£¾åŒº'},
    'edogawa': {'pages': 10, 'name': 'æ±Ÿæˆ¸å·åŒº'},
}

CRAWL_INTERVAL = 3.0  # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–“éš”

def save_property(url, session, scraper):
    """URLã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’å–å¾—ã—ã¦ä¿å­˜"""
    try:
        # source_idã‚’æŠ½å‡º
        source_id = url.split('/nc_')[1].split('/')[0] if '/nc_' in url else None
        if not source_id:
            return "skip"
        
        # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
        existing = session.query(Property).filter_by(source_id=source_id).first()
        if existing:
            return "exists"
        
        # è©³ç´°å–å¾—
        detail = scraper.get_property_detail(url)
        if not detail or not detail.get('price'):
            return "error"
        
        # æ–°è¦ä¿å­˜
        property_obj = Property(
            source='SUUMO',
            source_id=source_id,
            url=url,
            title=detail.get('title') or f'ç‰©ä»¶ {source_id}',
            price=detail.get('price'),
            area=detail.get('area'),
            price_per_sqm=detail.get('price_per_sqm'),
            layout=detail.get('layout'),
            building_age=detail.get('building_age'),
            floor=detail.get('floor'),
            direction=detail.get('direction'),
            address=detail.get('address'),
            prefecture=detail.get('prefecture'),
            city=detail.get('city'),
            station_name=detail.get('station_name'),
            station_distance=detail.get('station_distance'),
            access_info=detail.get('access_info'),
            management_fee=detail.get('management_fee'),
            repair_reserve=detail.get('repair_reserve'),
            features=detail.get('features', '{}'),
            is_active=True
        )
        
        session.add(property_obj)
        session.commit()
        return "saved"
        
    except Exception as e:
        print(f"      âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        session.rollback()
        return "error"

def process_area(area_code, config, session, scraper):
    """åŒºã”ã¨ã«ãƒšãƒ¼ã‚¸ã‚’å·¡å›ã—ã€è¦‹ã¤ã‘æ¬¡ç¬¬ä¿å­˜"""
    base_url = f'https://suumo.jp/ms/chuko/tokyo/sc_{area_code}/'
    pages = config['pages']
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    
    saved_count = 0
    
    for page in range(1, pages + 1):
        try:
            url = base_url if page == 1 else f"{base_url}?page={page}"
            print(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ {page}/{pages} ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
            
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code != 200:
                print(f"    âŒ HTTP {response.status_code}")
                continue
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.find_all('a', href=True)
            
            page_urls = set()
            for link in links:
                href = link['href']
                if '/ms/chuko/tokyo/' in href and '/nc_' in href:
                    if not href.startswith('http'):
                        href = 'https://suumo.jp' + href
                    if 'bukkengaiyo' not in href:
                        href = href.split('?')[0].rstrip('/') + '/bukkengaiyo/'
                    page_urls.add(href)
            
            print(f"    ğŸ” {len(page_urls)}ä»¶ã®URLã‚’ç™ºè¦‹ã€‚ä¿å­˜é–‹å§‹...")
            
            for p_url in page_urls:
                result = save_property(p_url, session, scraper)
                if result == "saved":
                    saved_count += 1
                    print(f"      âœ… ä¿å­˜æˆåŠŸ: {p_url.split('/nc_')[1].split('/')[0]}")
                elif result == "exists":
                    pass # å†—é•·ãªã®ã§å‡ºåŠ›ã—ãªã„
            
            time.sleep(CRAWL_INTERVAL)
            
        except Exception as e:
            print(f"    âš ï¸ ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
            continue
            
    return saved_count

def main():
    print("=" * 60)
    print("ğŸš€ è¶…é«˜é€Ÿã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«åé›†ï¼ˆç›®æ¨™: 500ä»¶ä»¥ä¸Šï¼‰")
    print("è¦‹ã¤ã‘æ¬¡ç¬¬DBã«ã‚³ãƒŸãƒƒãƒˆã—ã¾ã™ã€‚Streamlitã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«ç¢ºèªå¯èƒ½")
    print("=" * 60)
    
    engine = get_engine()
    session = get_session(engine)
    scraper = SuumoScraper(interval=1.0) # åŠ é€Ÿ
    
    total_saved = 0
    
    try:
        for idx, (area_code, config) in enumerate(AREAS.items(), 1):
            print(f"\n[{idx}/23] {config['name']} ã®å‡¦ç†ã‚’é–‹å§‹")
            count = process_area(area_code, config, session, scraper)
            total_saved += count
            print(f"  âœ¨ {config['name']} å®Œäº†: +{count}ä»¶ (åˆè¨ˆ: {total_saved}ä»¶)")
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    finally:
        session.close()
        print("\n" + "=" * 60)
        print(f"ğŸ çµ‚äº†ã€‚ä»Šå›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã®æ–°è¦ä¿å­˜: {total_saved}ä»¶")
        print("=" * 60)

if __name__ == '__main__':
    main()

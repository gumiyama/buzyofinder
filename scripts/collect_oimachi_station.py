#!/usr/bin/env python
"""
å¤§äº•ç”ºé§…ï¼ˆek_05130ï¼‰æŒ‡å®šã®é›†ä¸­åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import os
import sys
import time
from pathlib import Path
import requests
from bs4 import BeautifulSoup

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.database import get_session, get_engine, Property, save_or_update_property
from src.scrapers.suumo_scraper import SuumoScraper

# å¤§äº•ç”ºé§…ã®è¨­å®š
STATIONS = {
    '05480': {'pages': 20, 'name': 'å¤§äº•ç”ºé§…'},
}

CRAWL_INTERVAL = 1.0

def save_property(url, session, scraper):
    """URLã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’å–å¾—ã—ã¦ä¿å­˜ã¾ãŸã¯æ›´æ–°"""
    try:
        source_id = url.split('/nc_')[1].split('/')[0] if '/nc_' in url else None
        if not source_id: return "skip"
        
        detail = scraper.get_property_detail(url)
        if not detail or not detail.get('price'): return "error"
        
        return save_or_update_property(session, detail, source_id)
        
    except Exception as e:
        print(f"      âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        return "error"

def process_station(ek_code, config, session, scraper):
    """é§…ã”ã¨ã«ãƒšãƒ¼ã‚¸ã‚’å·¡å›"""
    base_url = f'https://suumo.jp/ms/chuko/tokyo/ek_{ek_code}/'
    pages = config['pages']
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    
    saved_count = 0
    
    for page in range(1, pages + 1):
        try:
            url = base_url if page == 1 else f"{base_url}?page={page}"
            print(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ {page}/{pages} ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­... ({config['name']})")
            
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code != 200:
                print(f"    âŒ HTTP {response.status_code}")
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            # æ¤œç´¢çµæœãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã®ã‚³ãƒ³ãƒ†ãƒŠã‚’ç‰¹å®šï¼ˆé€šå¸¸ 'property_unit' ã‚¯ãƒ©ã‚¹ãªã©ã‚’æŒã¤ï¼‰
            # ã‚‚ã—ãã¯ã€ãŠã™ã™ã‚ç‰©ä»¶ã‚’é™¤å¤–ã™ã‚‹ãŸã‚ã«ã€ç‰¹å®šã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ã¿ã‚’æ¢ç´¢
            main_content = soup.find('div', id='js-bukkenList') or soup
            links = main_content.find_all('a', href=True)
            
            page_urls = set()
            for link in links:
                href = link['href']
                # é§…è¿‘è¾ºã®æ¤œç´¢çµæœã«é™å®š
                if '/ms/chuko/tokyo/' in href and '/nc_' in href:
                    if not href.startswith('http'):
                        href = 'https://suumo.jp' + href
                    if 'bukkengaiyo' not in href:
                        href = href.split('?')[0].rstrip('/') + '/bukkengaiyo/'
                    page_urls.add(href)
            
            print(f"    ğŸ” {len(page_urls)}ä»¶ã®URLã‚’ç™ºè¦‹ã€‚ä¿å­˜é–‹å§‹...")
            
            for p_url in page_urls:
                result = save_property(p_url, session, scraper)
                if result == "saved":
                    saved_count += 1
                    print(f"      âœ… ä¿å­˜æˆåŠŸ: {p_url.split('/nc_')[1].split('/')[0]}")
                elif result == "exists":
                    pass
            
            time.sleep(CRAWL_INTERVAL)
            
        except Exception as e:
            print(f"    âš ï¸ ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
            continue
            
    return saved_count

def main():
    print("=" * 60)
    print("ğŸš€ å¤§äº•ç”ºé§… é›†ä¸­åé›†ï¼ˆé§…æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ï¼‰")
    print("=" * 60)
    
    engine = get_engine()
    session = get_session(engine)
    scraper = SuumoScraper(interval=1.0)
    
    total_saved = 0
    
    try:
        for ek_code, config in STATIONS.items():
            print(f"\n{config['name']} ã®å‡¦ç†ã‚’é–‹å§‹")
            count = process_station(ek_code, config, session, scraper)
            total_saved += count
            print(f"  âœ¨ {config['name']} å®Œäº†: +{count}ä»¶")
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    finally:
        session.close()
        print("\n" + "=" * 60)
        print(f"ğŸ çµ‚äº†ã€‚ä»Šå›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã®æ–°è¦ä¿å­˜: {total_saved}ä»¶")

if __name__ == '__main__':
    main()

#!/usr/bin/env python
"""
è‡ªå‹•ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
5åˆ†æ¯ã«æ–°è¦ç‰©ä»¶ã‚’åé›†ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ 
"""

import os
import sys
import time
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.scrapers.suumo_scraper import SuumoScraper
from src.models.database import get_session, get_engine, Property

# ã‚¨ãƒªã‚¢ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³# ã‚¨ãƒªã‚¢è¨­å®š
AREAS = {
    'chiyoda': {'pages': 5, 'name': 'åƒä»£ç”°åŒº', 'url': 'https://suumo.jp/ms/chuko/tokyo/sc_chiyoda/'},
    'shibuya': {'pages': 5, 'name': 'æ¸‹è°·åŒº', 'url': 'https://suumo.jp/ms/chuko/tokyo/sc_shibuya/'},
    'minato': {'pages': 5, 'name': 'æ¸¯åŒº', 'url': 'https://suumo.jp/ms/chuko/tokyo/sc_minato/'},
    'shinjuku': {'pages': 5, 'name': 'æ–°å®¿åŒº', 'url': 'https://suumo.jp/ms/chuko/tokyo/sc_shinjuku/'},
    'meguro': {'pages': 5, 'name': 'ç›®é»’åŒº', 'url': 'https://suumo.jp/ms/chuko/tokyo/sc_meguro/'},
}

INTERVAL = 1 * 60  # 1åˆ†ï¼ˆç§’å˜ä½ï¼‰
CRAWL_INTERVAL = 3.0  # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–“éš”


def collect_urls_from_area(area_config):
    """ã‚¨ãƒªã‚¢ã‹ã‚‰ç‰©ä»¶URLã‚’åé›†"""
    scraper = SuumoScraper()
    urls = set()
    
    for page in range(1, area_config['pages'] + 1):
        try:
            if page == 1:
                url = area_config['url']
            else:
                url = f"{area_config['url']}?page={page}"
            
            html = scraper._fetch_html(url)
            if not html:
                continue
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link['href']
                if '/ms/chuko/tokyo/' in href and '/nc_' in href:
                    if not href.startswith('http'):
                        href = 'https://suumo.jp' + href
                    if 'bukkengaiyo' not in href:
                        href = href.split('?')[0]
                        if not href.endswith('/'):
                            href += '/'
                        href += 'bukkengaiyo/'
                    urls.add(href)
            
            time.sleep(CRAWL_INTERVAL)
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    return urls


def save_property(url, session):
    """ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦DBã«ä¿å­˜"""
    scraper = SuumoScraper()
    
    try:
        detail = scraper.get_property_detail(url)
        if not detail or not detail.get('price'):
            return False
        
        # source_idã‚’æŠ½å‡º
        source_id = url.split('/nc_')[1].split('/')[0] if '/nc_' in url else None
        if not source_id:
            return False
        
        # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
        existing = session.query(Property).filter_by(source_id=source_id).first()
        if existing:
            return False  # ã‚¹ã‚­ãƒƒãƒ—
        
        # æ–°è¦ä¿å­˜
        property_obj = Property(
            source='SUUMO',
            source_id=source_id,
            url=url,
            title=detail.get('title') or f'ç‰©ä»¶ {source_id}',
            price=detail.get('price'),
            area=detail.get('area'),
            price_per_sqm=detail.get('price_per_sqm'),
            layout=detail.get('layout'),
            building_age=detail.get('building_age'),
            floor=detail.get('floor'),
            direction=detail.get('direction'),
            address=detail.get('address'),
            prefecture=detail.get('prefecture'),
            city=detail.get('city'),
            station_name=detail.get('station_name'),
            station_distance=detail.get('station_distance'),
            management_fee=detail.get('management_fee'),
            repair_reserve=detail.get('repair_reserve'),
            features=str(detail.get('features', {})),
            is_active=True
        )
        
        session.add(property_obj)
        session.commit()
        return True
        
    except Exception as e:
        print(f"  ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def auto_collect_cycle():
    """1ã‚µã‚¤ã‚¯ãƒ«ã®è‡ªå‹•åé›†"""
    print("\n" + "=" * 60)
    print(f"è‡ªå‹•åé›†é–‹å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    session = get_session(get_engine())
    total_new = 0
    
    for area in AREAS:
        print(f"\nğŸ“ {area['name']} ã‹ã‚‰åé›†ä¸­...")
        urls = collect_urls_from_area(area)
        print(f"  URLç™ºè¦‹: {len(urls)}ä»¶")
        
        new_count = 0
        for url in urls:
            if save_property(url, session):
                new_count += 1
                print(f"  âœ“ æ–°è¦ä¿å­˜ [{new_count}ä»¶ç›®]")
            time.sleep(CRAWL_INTERVAL)
        
        total_new += new_count
        print(f"  {area['name']}: {new_count}ä»¶è¿½åŠ ")
    
    # ç¾åœ¨ã®ç·ä»¶æ•°
    total_count = session.query(Property).filter_by(is_active=True).count()
    
    print("\n" + "=" * 60)
    print(f"ã‚µã‚¤ã‚¯ãƒ«å®Œäº†: {total_new}ä»¶è¿½åŠ ")
    print(f"ç·ç‰©ä»¶æ•°: {total_count}ä»¶")
    print("=" * 60)
    
    session.close()


def main():
    print("=" * 60)
    print("ğŸ¤– è‡ªå‹•ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ")
    print("1åˆ†æ¯ã«æ–°è¦ç‰©ä»¶ã‚’è‡ªå‹•åé›†")
    print("åœæ­¢ã™ã‚‹ã«ã¯ Ctrl+C ã‚’æŠ¼ã—ã¦ãã ã•ã„")
    print("=" * 60)
    
    cycle_count = 0
    
    try:
        while True:
            cycle_count += 1
            print(f"\nã€ã‚µã‚¤ã‚¯ãƒ« {cycle_count}ã€‘")
            
            auto_collect_cycle()
            
            # æ¬¡ã®ã‚µã‚¤ã‚¯ãƒ«ã¾ã§å¾…æ©Ÿ
            next_time = datetime.now()
            next_time = next_time.replace(second=0, microsecond=0)
            from datetime import timedelta
            next_time += timedelta(minutes=1)
            
            print(f"\nâ° æ¬¡å›å®Ÿè¡Œ: {next_time.strftime('%H:%M')}")
            print(f"   å¾…æ©Ÿä¸­... (1åˆ†)")
            
            time.sleep(INTERVAL)
            
    except KeyboardInterrupt:
        print("\n\nåœæ­¢ã—ã¾ã—ãŸã€‚")
        print(f"å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•°: {cycle_count}")


if __name__ == '__main__':
    main()

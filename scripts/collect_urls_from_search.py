"""
SUUMOã®æ¤œç´¢ãƒšãƒ¼ã‚¸ã‹ã‚‰ç‰©ä»¶URLã‚’è‡ªå‹•åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import time
import re
from bs4 import BeautifulSoup
import requests

# è¨­å®š
BASE_URL = "https://suumo.jp"
INTERVAL = 3.0  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰

# ã‚¨ãƒªã‚¢åˆ¥ã®æ¤œç´¢URLï¼ˆä¸­å¤ãƒãƒ³ã‚·ãƒ§ãƒ³ï¼‰
# æ³¨: ã“ã‚Œã¯ä»®ã®URLã§ã™å®Ÿéš›ã«ã¯ãƒ–ãƒ©ã‚¦ã‚¶ã§æ¤œç´¢ã—ã¦æ­£ã—ã„URLã‚’ç¢ºèªã—ã¦ãã ã•ã„
SEARCH_URLS = {
    'all_tokyo': 'https://suumo.jp/ms/chuko/tokyo/city/',  # æ±äº¬éƒ½å…¨åŸŸ
    'chiyoda': 'https://suumo.jp/ms/chuko/tokyo/sc_chiyoda/',  # åƒä»£ç”°åŒº  
    'shibuya': 'https://suumo.jp/ms/chuko/tokyo/sc_shibuya/',  # æ¸‹è°·åŒº
    'shinjuku': 'https://suumo.jp/ms/chuko/tokyo/sc_shinjuku/',  # æ–°å®¿åŒº
    'minato': 'https://suumo.jp/ms/chuko/tokyo/sc_minato/',  # æ¸¯åŒº
    'meguro': 'https://suumo.jp/ms/chuko/tokyo/sc_meguro/',  # ç›®é»’åŒº
}


def collect_urls_from_page(url, max_pages=5):
    """
    æ¤œç´¢ãƒšãƒ¼ã‚¸ã‹ã‚‰ç‰©ä»¶URLã‚’åé›†
    
    Args:
        url: æ¤œç´¢ãƒšãƒ¼ã‚¸ã®URL
        max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
    
    Returns:
        ç‰©ä»¶URLã®ãƒªã‚¹ãƒˆ
    """
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    })
    
    all_urls = []
    
    for page in range(1, max_pages + 1):
        # ãƒšãƒ¼ã‚¸ç•ªå·ä»˜ãURL
        page_url = f"{url}&page={page}" if page > 1 else url
        
        print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page}/{max_pages} ã‚’å–å¾—ä¸­...")
        
        try:
            response = session.get(page_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'lxml')
            
            # ç‰©ä»¶ã‚«ãƒ¼ãƒ‰ã‹ã‚‰è©³ç´°ãƒšãƒ¼ã‚¸URLã‚’æŠ½å‡º
            property_cards = soup.find_all('div', class_='property_unit')
            
            count = 0
            for card in property_cards:
                # h2.property_unit-title a ã‹ã‚‰URLã‚’å–å¾—
                title_elem = card.find('h2', class_='property_unit-title')
                if title_elem:
                    link = title_elem.find('a')
                    if link and 'href' in link.attrs:
                        property_url = link['href']
                        
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        if property_url.startswith('/'):
                            property_url = BASE_URL + property_url
                        
                        # /nc_XXXXX/ ã®å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯
                        if '/nc_' in property_url:
                            all_urls.append(property_url)
                            count += 1
            
            print(f"  âœ“ {count}ä»¶ã®ç‰©ä»¶URLã‚’ç™ºè¦‹")
            
            if count == 0:
                print(f"  ã“ã‚Œä»¥ä¸Šç‰©ä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                break
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(INTERVAL)
            
        except Exception as e:
            print(f"  âš  ã‚¨ãƒ©ãƒ¼: {e}")
            break
    
    return all_urls


def main():
    print("=" * 60)
    print("SUUMOç‰©ä»¶URLåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # ã‚¨ãƒªã‚¢é¸æŠ
    print("\nğŸ“ åé›†ã™ã‚‹ã‚¨ãƒªã‚¢ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print("  1. åƒä»£ç”°åŒº")
    print("  2. æ¸‹è°·åŒº")
    print("  3. æ–°å®¿åŒº")
    print("  4. æ¸¯åŒº")
    print("  5. ç›®é»’åŒº")
    print("  6. ã™ã¹ã¦")
    
    choice = input("\né¸æŠ (1-6): ").strip()
    
    area_map = {
        '1': ['chiyoda'],
        '2': ['shibuya'],
        '3': ['shinjuku'],
        '4': ['minato'],
        '5': ['meguro'],
        '6': ['chiyoda', 'shibuya', 'shinjuku', 'minato', 'meguro']
    }
    
    if choice not in area_map:
        print("âš  ç„¡åŠ¹ãªé¸æŠã§ã™")
        return
    
    areas = area_map[choice]
    
    # ãƒšãƒ¼ã‚¸æ•°å…¥åŠ›
    max_pages = input("\nå„ã‚¨ãƒªã‚¢ã§å–å¾—ã™ã‚‹ãƒšãƒ¼ã‚¸æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5): ").strip()
    max_pages = int(max_pages) if max_pages.isdigit() else 5
    
    # URLåé›†
    all_property_urls = []
    
    for area in areas:
        print(f"\nğŸ™ï¸ {area} ã®ç‰©ä»¶URLã‚’åé›†ä¸­...")
        urls = collect_urls_from_page(SEARCH_URLS[area], max_pages)
        all_property_urls.extend(urls)
        print(f"  åˆè¨ˆ {len(urls)}ä»¶")
    
    # é‡è¤‡å‰Šé™¤
    unique_urls = list(set(all_property_urls))
    
    print("\n" + "=" * 60)
    print(f"åé›†å®Œäº†ï¼åˆè¨ˆ {len(unique_urls)}ä»¶ã®ç‰©ä»¶URL")
    print("=" * 60)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = 'collected_property_urls.txt'
    with open(output_file, 'w', encoding='utf-8') as f:
        for url in unique_urls:
            f.write(url + '\n')
    
    print(f"\nğŸ’¾ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print(f"  python scripts/fetch_from_url_file.py {output_file}")


if __name__ == '__main__':
    main()
